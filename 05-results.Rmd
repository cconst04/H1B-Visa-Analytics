# Results
```{r, echo=FALSE}
library(dplyr)
library(mi)
library(ggplot2)
library(naniar)
library(extracat)
library(lubridate)
library(tidyverse)
library(forcats)
library(choroplethr)
library(choroplethrMaps)
library(arules)
library(treemap)
JOB_CODES <- c('11'='Management', '13'='Financial Operations', '23'='Legal',
               '15'='Computer and Maths', '16'='Architecture and Engineering',
               '17'='Life', '18'='Physical', '19'='Life, Physical, and Social Science',
               '21'='Community and Social Service', '25'='Educational Instruction and Library', 
               '27'='Arts, Design, Entertainment, Sports, and Media', '29'='Healthcare',
               '31'='Healthcare Support', '33'='Protective Service', '35'='Food Preparation and Serving',
               '37'='Building, Cleaning and Maintenance', '39'='Personal Care', '41'='Sales', 
               '43'='Office and Administrative Support', '45'='Farming, Fishing and Forestry',
               '47'='Construction and Extraction Occupations', '49'='Installation, Maintenance and Repair',
               '51'='Production Occupations', '53'='Transportation and Material Moving', '55'='Military'
              )
START_YEAR <- 2009
END_YEAR <- 2020
```
```{r Loading datasets}
test <- read.csv('data/raw/lca/2010.csv')
test <- test[test$LCA_CASE_EMPLOYER_STATE %in% state.abb, ]
df_ucsis <- data.frame()
for(year in START_YEAR:2020){
  tmp <- read.csv(paste0("data/raw/ucsis/", year, ".csv"))
  df_ucsis <- rbind(df_ucsis, tmp)
}
df_ucsis[df_ucsis==""] <- NA
count_applications <- list()
#read all years
df <- data.frame()
for(year in START_YEAR:END_YEAR){
  tmp <- read.csv(paste0("data/clean/lca/", year, ".csv"))
  tmp$year <- year
  df <- rbind(df, tmp)
  count_applications[[year - START_YEAR + 1]] <- nrow(data.frame(tmp))
}
df$FULL_TIME_POSITION.1 <- NULL
df$SOC_CODE <- as.character(df$SOC_CODE)
#remove 0 wage
df <- df[df$annual_wage > 0  && !is.nan(df$annual_wage),]

num_applications <- data.frame(year=START_YEAR:END_YEAR)
num_applications['total_applications'] <- unlist(count_applications)
df_tmp <- df
df_tmp <- df_tmp[!is.na(df_tmp$SOC_CODE), ]
df_tmp <- df_tmp[df_tmp$SOC_CODE %in% names(JOB_CODES),]
counts <- count(df_tmp, year, SOC_CODE)
counts <- transform(counts, SOC_CODE = JOB_CODES[SOC_CODE])
counts <- counts[!is.na(counts$SOC_CODE),]

```
  
## Number of applications per year  
```{r Applications per year, width=10, height=10}
ggplot(data=num_applications, aes(x=year, y=total_applications)) +
  geom_line() + labs(y= "Number of applications", x = "Year")
```   
  
There is a big increase in the number of applications from 2009 to 2016. After that, number of applications fluctuate and they reach 590000 applicants in 2020. This indicates that US gets more and more popular option for skilled workers that want to migrate, probably because of the attractive salaries that increase year by year, which is further analyzed in the next figure  
  
## Annual salary boxplot    
```{r, width=10, height=10}
boxplot(annual_wage ~ year, data=df, outline=FALSE, ylab='Yearly Salary')
```  
  
There is an upward trend in the 25th quartile, median, 75th quartile and maximum salary from 2009 to 2020.  We can't say anything about the minimum values because of outliers, even though salaries equal to 0 were removed. We can't remove a bigger value, because each state has different rate of pay and a minimum salary to filter isn't a clear cut   
  
## Applications by subject  
```{r, fig.width=12, fig.height=20}
ggplot(counts, aes(x=n, y=SOC_CODE)) + geom_col() + facet_wrap(~ year, ncol=2) + labs(y= "Job sector", x = "Number of applications")
```  
Jobs related to computer science and mathematics are the most famous in the H-1B applications every year. There is an increasing trend in the applications of this field from 2009 to 2015 After that the number of applications remains constant at 400000 applications per year. This also applies for the rest of the sectors and confirmed by the number of applications vs year graph  
  
```{r, echo=FALSE, width=10, height=10}
counts <- counts[counts$SOC_CODE != 'Computer and Maths',]
```  
  
## Applications by subject with Computer Science and Maths removed  
```{r, fig.width=12, fig.height=20}
ggplot(counts, aes(x=n, y=SOC_CODE)) + geom_col() + facet_wrap(~ year, ncol=2) +
  labs(y= "Job sector", x = "Number of applications")
```  
  
Financial operations is the second most famous job in the H-1B applications every year. There is an initial increase in the number of applications between 2009 and 2012. In 2013 the number of applicants is similar to that of the previous year, followed by an increase in 2014. From 2015 to 2019 total applications in this sector remain constant with around 480000 applicants. In 2020 there is a reduction, with total 400000 applicants. It is worth mentioning that number of applications for each sector between 2011 and 2020 are very similar. This may be because have fixed number of positions for international workers that they are applying for H-1B visa.  For the rest of sectors there is an increase between 2009 and 2011. In 2011 there is a big decrease in the management and community with social services related job applications. For the next 3 years there is a slight fluctuation in each sector. Finally, from 2015 to 2020 the number of applications for the rest of the sector is constant


```{r, echo=FALSE, eval=FALSE}
df <- read.csv("data/processed/processed_2020.csv") #Using most recent data
states <- read.csv("data/external/states.csv")
states_population <- read.csv("data/raw/state_population.csv")
states <- left_join(states,states_population, by=c("State"="NAME"))
df <- df %>% filter(EMPLOYER_STATE %in% states$Code)
soc_codes = read.csv("data/processed/soc_codes.csv")
naics_codes = read.csv("data/processed/naics_sectors.csv")
```
### Top Employers

Lets look at the top 20 Companies overall in 2020 filing H1-B Visa applications. This means across all industries and jobs.
```{r, fig.width=10, eval=FALSE}
top_companies = df %>% group_by(EMPLOYER_NAME) %>%  dplyr::summarise(Frequency = n()) %>% arrange(desc(Frequency))

top_companies[1:20,] %>% 
  ggplot(aes(y = fct_rev(fct_inorder(factor(EMPLOYER_NAME))),x = Frequency)) + 
  geom_col(fill="lightblue",color = "black") +
  labs(y = "Employer Name", title = "Top H1-B Visa Sponsors in the U.S.", x = "Number of H1-B applications filed")

```

The top 20 is dominated by Tech and Consulting companies (primarily). However, it's worth noticing that these companies are also very big in terms of their worth and more importantly, their number of employees. For example, Cognizant has 289.000 employees, Infosys 242.000, Tata Consultancy has 470.000, Google 135.000. Nevertheless, there are large companies in all industries, but for some reason tech and consulting stand out. For a future work, it might be interesting to normalize these frequencies by the company size to get a better sense of the probability that a company sponsors an H1B visa for an employee.


### Geographical analysis

Now we want to see if there are any geographical patterns. To do this we calculated the number of H1-B application per 1M people and plot
```{r fig.width= 13, eval=FALSE}
tidydf <- df %>% 
  group_by(EMPLOYER_STATE) %>% 
  dplyr::summarise(value = n()) %>% 
  filter(EMPLOYER_STATE %in% states$Code)

tidydf<- left_join(tidydf,states,by = c("EMPLOYER_STATE" = "Code")) %>% mutate(region = tolower(State))
tidydf <- tidydf %>% mutate(value = value*1000000/tidydf$POPESTIMATE2020)

state_choropleth(as.data.frame(tidydf),title  = "H1B visa requests in the US", 
                 legend = "Applications per 1M people")
```

It looks that there are high concentrations on the coasts. Woud the percentage of immigrant population by state have something to do with this? Apparently, yes! After doing some research, we found that some of the top states by immigrant population (percentage) include:

* California (27%)
* New York (23%)
* New Jersey (22%)
* Texas (17%)
* Washington (15%)
* Massachusetts (17%)
* Illinois (14%)
* Maryland (15%)

On the other hand, the states with least immigrants are:

* Montana (2%)
* Mississippi (2.5%)
* Alabama (3%)
* South Dakota (4%)

Of course, immigration is not a perfect predictor for the number of applications. There are other factor into play like the presence of popular industries (e.g., Tech, Finance, Consulting). Another factor may be the wealth of the state measured by houshold income which would help explain some of the variance. For example, Nevada and Arizona are in the top 15 of the states with most immigrants but their percentage of applications is not as high as expected, nonetheless, these states are in the 28th and 30th place according to household income. On the other hand, the top "richest" states agree with the ones with highes immigrants and highest visa applications.

Note: data about immigrant population is based on: https://en.wikipedia.org/wiki/List_of_U.S._states_and_territories_by_immigrant_population and https://worldpopulationreview.com/state-rankings/states-with-the-most-immigrants. Data about household income was found here: https://en.wikipedia.org/wiki/List_of_U.S._states_and_territories_by_income


### Employer Industry and Job Category Composition

How is the demand for international workers across different indusrties? In other words, what job categories are in high demand for the different industry sectors? Here we present a treemap that shows the top 4 job categories for the top 5 industry sectors. We can't show all jobs and industries for visualization purposes.

```{r fig.width=12, fig.height=12, eval=FALSE}
get_num_digits <-function(x){
                  num_digits = floor (log10 (abs (x))) + 1
                  return(num_digits)
}

strict_substr <-function(df,column,start,finish){
  return(if_else(str_length(df[,column]) >=finish, substr(df[,column],start,finish),NULL))
}

df$NAICS_CODE <- as.character(df$NAICS_CODE)

df$naics_sector <-strict_substr(df,"NAICS_CODE",1,2)
df$industry_group <- strict_substr(df,"NAICS_CODE",1,4)
df$naics_industry <- strict_substr(df,"NAICS_CODE",1,5)
df$national_industry <- strict_substr(df,"NAICS_CODE",1,6)


naics_codes$industry_group <- as.character(naics_codes$industry_group)
df <- left_join(df,naics_codes,by="industry_group")
df <- left_join(df,soc_codes,by = "SOC_CODE")


top_n_industries = 5
top_n_jobs = 4

top_industries <- (df %>%
                    group_by(Title) %>% 
                    dplyr::summarise(Frequency = n()) %>% 
                    arrange(desc(Frequency)) %>%
                    head(top_n_industries) )['Title']


top_ind_job <- df %>% filter(Title %in% top_industries$Title) %>%
                group_by(Title,job_category) %>% 
                dplyr::summarise(Frequency = n()) %>% arrange(Title,desc(Frequency) )%>% 
                group_by(Title) %>% slice_head(n = top_n_jobs)


treemap(top_ind_job,
        index = c('Title','job_category'),
        vSize = 'Frequency',
        type = 'index',
    fontsize.labels=c(10,8),                # size of labels. Give the size per level of aggregation: size for group, size for subgroup, sub-subgroups...
    fontcolor.labels=c("white","black"),    # Color of labels
    fontface.labels=c(2,1),                  # Font of labels: 1,2,3,4 for normal, bold, italic, bold-italic...
    bg.labels=c("black"),              # Background color of labels
    align.labels=list(
        c("left", "top"), 
        c("right", "bottom")
        ),                                   # Where to place labels in the rectangle?
    overlap.labels=0.1,                      # number between 0 and 1 that determines the tolerance of the overlap between labels. 0 means that labels of lower levels are not printed if higher level labels overlap, 1  means that labels are always printed. In-between values, for instance the default value .5, means that lower level labels are printed if other labels do not overlap with more than .5  times their area size.
    inflate.labels=F,
    palette = 'Pastel1',
    title = "Composition of top company industries (highlighted in black) and job categories")                       # If true, labels are bigger when rectangle is bigger.

```

It is very clear that industries belonging to the STEM (Science, Technology, Engineering and Mathematics) fields dominate in visa applications. This is great news for Data Scientists like us because it means companies in our industry are more than happy to sponsor us (what a relief). Additionally, the top job categories in each industry sector remain pretty much the same:

* Computer and Mathematical Occupationis
* Business and Financial Operations Occupations
* Management Occupations
* Architecture and Engineerring Occupations

Nevertheless, there is something surprising: the third industry with most H1-B applications is Colleges, Universities and Professional Schools. This means that these sectors are very interested in international workers and would then be great places to apply for jobs. We couldn't resist to give a peek at the top employers in this last sector:

```{r, eval=FALSE}
df[df$Title == "Colleges, Universities, and Professional Schools",] %>% 
  group_by(EMPLOYER_NAME) %>%
  dplyr::summarise(Frequency = n()) %>%
  arrange(desc(Frequency)) %>% 
  drop_na()
```

We are so proud to see Columbia University at the top! We are at a great place to learn but also with substantial chances of employment.


### Wage analysis

How are H1-B Visa applicants being paid? Our hypothesis is that they should be getting a more than average wage because this visa is intended for "speciallized workers".

We downloaded the latest personal income information provided by the U.S. Census in order to compare the general US citizen wages with that of the people applying for H1-B visas.
Let's look at the data. Note, this file was cleaned to extract the lower and upper wage bounds 

```{r, eval=FALSE}
personal_income=read.csv("../data/processed/personal_income.csv")
personal_income
```

These income ranges are too specific for this analysis so we decided to group by ranges of $25K, except for the last category which is still 100K or more. 

```{r, eval=FALSE}

group_size = 25000
num_breaks = (max(personal_income$lower_bound)-min(personal_income$lower_bound)+1)/group_size
breaks  = c(seq(0,100000,group_size),100000.0001)

personal_income$new_groups <-discretize(personal_income$lower_bound,method = "fixed",breaks=breaks,infinity = TRUE)
personal_income$income_group <- fct_recode(personal_income$new_groups,
                                               "Less than $25K"="[-Inf,2.5e+04)",
                                               "$25K to $50K" = "[2.5e+04,5e+04)",
                                               "$50K to $75K" = "[5e+04,7.5e+04)",
                                               "$75K to $100K" = "[7.5e+04,1e+05)",
                                               "$100K or more" = "[1e+05, Inf]")
personal_income
```
Now lets create the same labels for our dataset

```{r, eval=FALSE}

df$income_group <-discretize(df$annual_wage,method = "fixed",breaks=breaks,infinity = TRUE)
df$income_group <- fct_recode(df$income_group,
                                               "Less than $25K"="[-Inf,2.5e+04)",
                                               "$25K to $50K" = "[2.5e+04,5e+04)",
                                               "$50K to $75K" = "[5e+04,7.5e+04)",
                                               "$75K to $100K" = "[7.5e+04,1e+05)",
                                               "$100K or more" = "[1e+05, Inf]")

head(df)
```

Finally, let's see how different are wage groups for H1-B applicants vs the general population.
```{r, eval=FALSE}
personal_income_by_wage_range <- personal_income %>% 
                          group_by(income_group) %>% 
                          dplyr::summarise(percentage = sum(frequency)*100/sum(personal_income$frequency)) 

personal_income_by_wage_range$type = "General"

df_by_wage_range <- df %>% 
                          group_by(income_group) %>% 
                          dplyr::summarise(percentage = n()*100/nrow(df)) 
df_by_wage_range$type = "H1-B"


tidydf <- rbind(personal_income_by_wage_range,df_by_wage_range)

#Thanks to https://edav.info/cleveland.html
theme_dotplot <- theme_bw(14) +
    theme(axis.text.y = element_text(size = rel(.75)),
        axis.ticks.y = element_blank(),
        axis.title.x = element_text(size = rel(.75)),
        panel.grid.major.x = element_blank(),
        panel.grid.major.y = element_line(size = 0.5),
        panel.grid.minor.x = element_blank())


ggplot(tidydf, aes(percentage, income_group ,color = type)) +
  geom_point() +
  labs(x = "Percentage of population", y = "Annual Income Range", title = "Wage Income Ranges", subtitle = "H1-B Applicants vs U.S. General Population" ) +
  theme_dotplot

```

The differences in salary are stagering! The U.S Nation median wage is in the 25K-50K range while that of H1-B applicants is in the 75K-100K range. This is most likely a reflection of the kind of people which H1-B visas are intended for: High-Skilled speciallized workers with at least a bachelor's degree. Furthermore, notice that the percentage of H1-B applicants that earn less than 50K is minimal. This could mean that "unspecialized" workers are not likely to have "low" wages, or that speciallized workers at low-paying are not willing to sponsor visas because they don't have the financial means (which is why they are paying low wages).


```{r, eval=FALSE}
top_n = 5
cols = c("SOC_CODE","EMPLOYER_STATE","EMPLOYER_NAME")
top_companies_by_soc_state <- df[,cols] %>% group_by(SOC_CODE,EMPLOYER_STATE,EMPLOYER_NAME) %>%
  dplyr::summarise(frequency = n()) %>%
  slice_max(order_by = frequency, n = top_n, with_ties = FALSE)

top_companies_by_soc_state <- left_join(top_companies_by_soc_state,states[c("Code","State")],by =c("EMPLOYER_STATE"= "Code"))

```
